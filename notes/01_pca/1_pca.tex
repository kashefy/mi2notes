\section{PCA}

\mode<presentation>{
\begin{frame} 
    \begin{center} \huge
        \secname
    \end{center}
    \begin{center}
    Transform and rotate the data s.t. keeping only the first $M$ dimensions has minimal MSE.
    \end{center}
\end{frame}
}

\subsection{Procedure and Projection}

\begin{frame}{Visible}\frametitle{\secname: \subsecname}

\begin{enumerate}
	\item<visible@1-> Center the data, $\E\lbrack\vec x\rbrack = \vec m  = \frac{1}{p} \sum_{\alpha=1}^{p} \vec x^{(\alpha)}\eqexcl \vec 0$.
\slidesonly{	\only<2>{
\svspace{5mm}
	\begin{center}
		\includegraphics[width=5cm]{img/meme_center}
    \end{center}
	}
}
	%\visible<3->
	%{
	\item<visible@3-> Let $\vec X$ be the $N \times p$ matrix of the centered data.
	\item<visible@4-> Measure
	\begin{itemize}
	\item the variance of each component in $\vec x$.\\
	\textbf{Not enough}, the variables in $\vec x$ could be correlated.
	\item the covariances $C_{ij} \;\; \forall\,i,j = 1,\ldots,N$.
	\item<visible@4->[$\Rightarrow$] Construct the covariance matrix $\vec C$.
		\begin{equation}
		\vec C = \text{Cov}(\vec X) = \mathbf{\Sigma} = \E\lbrack\vec X~\vec X^\top\rbrack \in \R^{N \times N}
		\end{equation}
	\end{itemize}
	\svspace{-5mm}
	\item<visible@6-> \textbf{eigenvalue decomposition}
	\item<visible@7-> Order eigenvalues in \emph{descending} order. (Highest variance first). The ordered eigenvectors are the \emph{principle components} of the dataset $\vec X$.
	\item<visible@8-> Project $\vec x$ onto the \textcolor{red}{first} $\color{red}M$ PCs.
	%}
\end{enumerate}


\end{frame}

\subsubsection{Eigenvalue decomposition}

\begin{frame}\frametitle{\subsubsecname}

\begin{equation}
\vec C \, \vec e_a \; = \; \lambda_a \vec e_a  \quad\text{(the eigenvalue problem of PCA)}
\end{equation}

\begin{itemize}
\item[] $\lambda_a: \text{the eigenvalue, the variance along principle component } a.$
\item[] $\vec e_a: \text{the \underline{normalized} eigenvector, the direction of the } a\text{-th PC in } \R^N$
\end{itemize}

\question{How do we perform an eigenvalue decomposition?}

\pause

\begin{enumerate}
\item Get eigenvalues:
\begin{equation}\det(\vec C-\lambda \vec I) = 0,
\end{equation} 
\item Find the eigenvector $\vec e_a$ associated with each $\lambda_a$ by solving the linear system
\footnote{If interested, see {\emph{math\_primer.pdf} on ISIS} for details and an example.}
\begin{equation}(\vec C - \lambda_a \vec I )\, \vec e_a = \vec 0
\end{equation}
\end{enumerate}

\end{frame}

\subsubsection{The Scree plot}

\begin{frame}\frametitle{\subsubsecname}

In PCA we sort the eigenvectors from highest to smallest eigenvalue.
Plotting the sorted eigenvalues is referred to as a \emph{scree plot}

\begin{center}
\includegraphics[width=0.6\textwidth]{img/screeplot_kpca_poly_d3}%
\captionof{figure}{Example of a scree plot}
\end{center}

\end{frame}

\subsubsection{Projection onto the PC space}

\begin{frame}\frametitle{\subsubsecname}

Recall \notesonly{from \sectionref{sec:objective} }that we wanted to \notesonly{improve on simple truncation by
transforming }\slidesonly{transform }the data before transmitting the first $M$ components.

\notesonly{We described the projection as follows:}

\begin{equation}
\vec u = \vec M^\top \vec x = (a_1,a_2,\ldots,a_N)^\top
\end{equation}


We can now define the transformation using the normalized eigenvectors of the PCs:

\begin{equation}
\vec M := (\vec e_1, \vec e_2, \ldots,\vec e_N)
\end{equation}

\slidesonly{

\begin{equation}
	\vec{x} = \underbrace{ a_1 }_{ \vec{e}_1^\top \vec{x} } \vec{e}_1
		+ \underbrace{ a_2 }_{ \vec{e}_2^\top \vec{x} } \vec{e}_2
		+ \ldots
		+ \underbrace{ a_N }_{ \vec{e}_N^\top \vec{x} } \vec{e}_N
\end{equation}
}


\end{frame}

\subsection{Reconstruction error}

\begin{frame}\frametitle{How much better is this vs. simple truncation of $\vec x$?}
%\newpage

\pause

\only<2,3>{
The transformation onto the PCs is linear:
\begin{equation}
	\vec{x} = \underbrace{ a_1 }_{ \vec{e}_1^\top \vec{x} } \vec{e}_1
		+ \underbrace{ a_2 }_{ \vec{e}_2^\top \vec{x} } \vec{e}_2
		+ \ldots
		+ \underbrace{ a_N }_{ \vec{e}_N^\top \vec{x} } \vec{e}_N
\end{equation}

Therefore, we can perfectly reconstruct observations by projecting them from PC space (i.e. feature space) back to the input space. \\
If we only use the \textcolor{red}{first} ${\color{red}M}$ PCs for reconstructing the observations, we will accumulate error.

\svspace{-5mm}

\begin{equation}
\widetilde{\vec{x}} = a_1 \vec{e}_1 + a_2 \vec{e}_2 + \ldots
		+ a_M \vec{e}_M
\end{equation}
}
		
\notesonly{We measure MSE between original centered observations and their corresponding reconstructions just as we did in \eqref{eq:mse}.}

\pause

\only<3,4>{
\begin{equation}
E = \frac{1}{p} \sum_{\alpha = 1}^{p} (\vec x - \widetilde{\vec x})^2 = \frac{1}{p} \sum_{\alpha = 1}^{p} \sum_{j = {\color{cyan}M+1}}^{{\color{cyan} N}} (a_j^{(\alpha)})^2
\end{equation}
}
\only<4>{
\svspace{5mm}

The MSE is equal to the sum of variances of the \textcolor{cyan}{last} $\notesonly{N-(M+1)-1=}{\color{cyan}N-M}$ components of the \emph{transformed} observations $\vec u=\vec M^\top \vec x$.\\

\svspace{5mm}

\notesonly{Since the PCs are ordered w.r.t to variance in descending order, }\notesonly{t}\slidesonly{T}he variances of the \textcolor{cyan}{last} $\color{cyan}N-M$ components of the transformed data are smallest.
The transformation is therefore optimal in the sense of minimal MSE.
}

\end{frame}
